{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0a930a",
   "metadata": {},
   "source": [
    "# IVFC Daily Operations Report\n",
    "**Station 187 - Ingomar Volunteer Fire Company**\n",
    "\n",
    "Automated daily report covering rolling daily, weekly, and monthly incident statistics, weather risk, traffic risk, and a duty-crew narrative generated by Claude AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once to install dependencies (or include in your Dockerfile)\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"requests\",\n",
    "    \"python-docx\",\n",
    "    \"anthropic\",\n",
    "    \"sendgrid\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"lxml\",\n",
    "    \"pytz\",\n",
    "    \"pandas\",\n",
    "    \"google-cloud-storage\",\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\").split(\"==\")[0])\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "\n",
    "print(\"All dependencies ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1cda0",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, io, json, base64, pytz, requests, warnings\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor, Inches, Cm\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "import anthropic\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "EASTERN = pytz.timezone(\"America/New_York\")\n",
    "NOW_ET   = datetime.now(EASTERN)\n",
    "TODAY    = NOW_ET.date()\n",
    "\n",
    "DAILY_START   = NOW_ET - timedelta(days=1)\n",
    "WEEKLY_START  = NOW_ET - timedelta(days=7)\n",
    "MONTHLY_START = NOW_ET - timedelta(days=30)\n",
    "\n",
    "DAYTIME_START_HOUR = 6\n",
    "DAYTIME_END_HOUR   = 18\n",
    "\n",
    "print(f\"Report generated: {NOW_ET.strftime('%Y-%m-%d %H:%M %Z')}\")\n",
    "print(f\"Daily window:     {DAILY_START.strftime('%Y-%m-%d %H:%M')} to now\")\n",
    "print(f\"Weekly window:    {WEEKLY_START.strftime('%Y-%m-%d %H:%M')} to now\")\n",
    "print(f\"Monthly window:   {MONTHLY_START.strftime('%Y-%m-%d %H:%M')} to now\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307ab57",
   "metadata": {},
   "source": [
    "## Section 2: Credentials and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRSTDUE_EMAIL    = os.environ.get(\"FIRSTDUE_EMAIL\",    \"peterethanmuller@gmail.com\")\n",
    "FIRSTDUE_PASSWORD = os.environ.get(\"FIRSTDUE_PASSWORD\", \"Apache2019!firstdue\")\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\", \"sk-ant-api03-xxxxxxxxxxx\")\n",
    "SENDGRID_API_KEY  = os.environ.get(\"SENDGRID_API_KEY\",  \"SG.xxxxxxxxxxx\")\n",
    "TOMTOM_API_KEY    = os.environ.get(\"TOMTOM_API_KEY\",    \"cDNrnThDx91mq5SPwfWQSR5BcQzoREfM\")\n",
    "GCS_BUCKET_NAME   = os.environ.get(\"GCS_BUCKET_NAME\",   \"ivfc-reports-187\")\n",
    "\n",
    "EMAIL_FROM = os.environ.get(\"EMAIL_FROM\", \"pmuller@andrew.cmu.edu\")\n",
    "EMAIL_TO   = os.environ.get(\"EMAIL_TO\",   \"pmuller@andrew.cmu.edu,dbaity@andrew.cmu.edu,afrocha@andrew.cmu.edu,yanxuand@andrew.cmu.edu\")\n",
    "EMAIL_CC   = os.environ.get(\"EMAIL_CC\",   \"pmuller@andrew.cmu.edu\")\n",
    "\n",
    "INCIDENT_CATEGORIES = [\n",
    "    (\"Commercial AFA\",            [\"FIRE ALARM - COMMERCIAL\", \"FIRE ALARM - HIGH LIFE HAZARD\",\n",
    "                                   \"FIRE ALARM - HIGH RISE\", \"FIRE ALARM - MULTI FAMILY\"]),\n",
    "    (\"Residential AFA\",           [\"FIRE ALARM - RESIDENTIAL\"]),\n",
    "    (\"Vehicle Crash w/ Injuries\", [\"CRASH - INJURIES\", \"ACCIDENT WITH INJURIES\"]),\n",
    "    (\"Crash w/ Entrapment\",       [\"CRASH - TRAPPED\", \"CRASH - PINNED\"]),\n",
    "    (\"Crash - Other/Unknown\",     [\"CRASH - UNKNOWN\", \"CRASH - HAZARDS\",\n",
    "                                   \"CRASH - HIGH MECHANISM\", \"CRASH - INVOLVING A STRUCTURE\"]),\n",
    "    (\"Natural Gas Odor/Leak\",     [\"NATURAL GAS\"]),\n",
    "    (\"Structure Fire\",            [\"RESIDENTIAL BLDG FIRE\", \"COMMERCIAL BLDG FIRE\",\n",
    "                                   \"POSSIBLE RES BLDG FIRE\", \"POSSIBLE COMM BLDG FIRE\",\n",
    "                                   \"FIRE - HIGH LIFE HAZARD\"]),\n",
    "    (\"Odor Investigation\",        [\"ODOR INVESTIGATION\", \"SMOKE INVESTIGATION\",\n",
    "                                   \"CARBON MONOXIDE - RESIDENTIAL\", \"CARBON MONOXIDE - COMMERCIAL\"]),\n",
    "    (\"Vehicle Fire\",              [\"VEHICLE FIRE\", \"VEHICLE FIRE - HIGHWAY\"]),\n",
    "    (\"Brush/Grass/Woods Fire\",    [\"BRUSH FIRE\", \"GRASS FIRE\", \"WOODS FIRE\", \"MULCH FIRE\"]),\n",
    "    (\"Electrical Hazard\",         [\"ELECTRICAL HAZARD\", \"ELECTRICAL FIRE\"]),\n",
    "    (\"Water Emergency\",           [\"WATER EMERGENCY\", \"FLOOD/WATER CONDITION\"]),\n",
    "    (\"Hazmat\",                    [\"HAZMAT\", \"FUEL SPILL - HIGHWAY\", \"FUEL SPILL\"]),\n",
    "    (\"Medical Emergency\",         [\"MEDICAL EMERGENCY\", \"EMS ASSIST\"]),\n",
    "    (\"Mutual Aid Given\",          [\"MUTUAL AID GIVEN\", \"MUTUAL AID - GIVEN\"]),\n",
    "    (\"Mutual Aid Received\",       [\"MUTUAL AID RECEIVED\", \"MUTUAL AID - RECEIVED\"]),\n",
    "    (\"Service Call\",              [\"SERVICE CALL\", \"ASSIST - POLICE\", \"ASSIST - OTHER AGENCY\",\n",
    "                                   \"WIRES DOWN\", \"TREE DOWN\", \"PUBLIC ASSIST\"]),\n",
    "    (\"Cancelled/No Activity\",     [\"CANCELLED EN ROUTE\", \"CANCELLED ON SCENE\",\n",
    "                                   \"NO ACTIVITY FOUND\", \"UNFOUNDED\"]),\n",
    "    (\"Other / Uncategorized\",     []),\n",
    "]\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  Anthropic key: {'set' if ANTHROPIC_API_KEY else 'MISSING'}\")\n",
    "print(f\"  TomTom key:    {'set' if TOMTOM_API_KEY else 'MISSING'}\")\n",
    "print(f\"  SendGrid key:  {'set' if SENDGRID_API_KEY else 'MISSING'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649ef89",
   "metadata": {},
   "source": [
    "## Section 3: First Due API (Authentication)\n",
    "> Currently commented out - using CSV fallback. Uncomment when API access is granted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstdue_get_token(email: str, password: str) -> str:\n",
    "    url = \"https://api.firstdue.com/fd-api/v1/auth/token\"\n",
    "    resp = requests.post(url, json={\"email\": email, \"password\": password}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"access_token\"]\n",
    "\n",
    "# FIRSTDUE_TOKEN = firstdue_get_token(FIRSTDUE_EMAIL, FIRSTDUE_PASSWORD)\n",
    "# FIRSTDUE_HEADERS = {\"Authorization\": f\"Bearer {FIRSTDUE_TOKEN}\"}\n",
    "FIRSTDUE_TOKEN = \"\"\n",
    "FIRSTDUE_HEADERS = {}\n",
    "print(\"First Due auth skipped - using CSV data source.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d959a3",
   "metadata": {},
   "source": [
    "## Section 4: First Due API (Fetch Dispatches)\n",
    "> Commented out - using CSV fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a605bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_dispatches(since_dt, headers: dict) -> list:\n",
    "    url = \"https://api.firstdue.com/fd-api/v1/dispatches\"\n",
    "    since_str = since_dt.astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    params = {\"since\": since_str}\n",
    "    all_items = []\n",
    "    while url:\n",
    "        resp = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        all_items.extend(data.get(\"data\", data if isinstance(data, list) else []))\n",
    "        link_header = resp.headers.get(\"Link\", \"\")\n",
    "        next_url = None\n",
    "        for part in link_header.split(\",\"):\n",
    "            part = part.strip()\n",
    "            if 'rel=\"next\"' in part:\n",
    "                match = re.search(r\"<([^>]+)>\", part)\n",
    "                if match:\n",
    "                    next_url = match.group(1)\n",
    "                    break\n",
    "        url = next_url\n",
    "        params = {}\n",
    "    return all_items\n",
    "\n",
    "# if FIRSTDUE_TOKEN:\n",
    "#     raw = fetch_all_dispatches(MONTHLY_START, FIRSTDUE_HEADERS)\n",
    "#     print(f\"Fetched {len(raw)} dispatches from First Due API.\")\n",
    "print(\"First Due fetch skipped - using CSV data source.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44268cda",
   "metadata": {},
   "source": [
    "## Section 5: Rolling Stats Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_window(df: pd.DataFrame, start: datetime) -> pd.DataFrame:\n",
    "    start_naive = start.replace(tzinfo=None)\n",
    "    return df[df[\"timestamp\"] >= start_naive].copy()\n",
    "\n",
    "def categorize(incident_type: str) -> str:\n",
    "    t = str(incident_type).strip().upper()\n",
    "    for cat_name, keywords in INCIDENT_CATEGORIES:\n",
    "        if cat_name == \"Other / Uncategorized\":\n",
    "            continue\n",
    "        for kw in keywords:\n",
    "            if kw.upper() in t:\n",
    "                return cat_name\n",
    "    return \"Other / Uncategorized\"\n",
    "\n",
    "def is_daytime(ts) -> bool:\n",
    "    return DAYTIME_START_HOUR <= ts.hour < DAYTIME_END_HOUR\n",
    "\n",
    "def build_incident_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    categories = [c[0] for c in INCIDENT_CATEGORIES]\n",
    "    day_counts  = {c: 0 for c in categories}\n",
    "    eve_counts  = {c: 0 for c in categories}\n",
    "    day_total = 0\n",
    "    eve_total = 0\n",
    "    for _, row in df.iterrows():\n",
    "        cat = row.get(\"category\", \"Other / Uncategorized\")\n",
    "        ts  = row[\"timestamp\"]\n",
    "        if is_daytime(ts):\n",
    "            day_counts[cat] = day_counts.get(cat, 0) + 1\n",
    "            day_total += 1\n",
    "        else:\n",
    "            eve_counts[cat] = eve_counts.get(cat, 0) + 1\n",
    "            eve_total += 1\n",
    "    rows = []\n",
    "    for cat in categories:\n",
    "        d = day_counts.get(cat, 0)\n",
    "        e = eve_counts.get(cat, 0)\n",
    "        if d > 0 or e > 0:\n",
    "            rows.append({\n",
    "                \"Category\":      cat,\n",
    "                \"Daytime\":       d,\n",
    "                \"Evening/Night\": e,\n",
    "                \"Total\":         d + e,\n",
    "            })\n",
    "    totals_row = {\n",
    "        \"Category\":      \"TOTAL\",\n",
    "        \"Daytime\":       day_total,\n",
    "        \"Evening/Night\": eve_total,\n",
    "        \"Total\":         day_total + eve_total,\n",
    "    }\n",
    "    rows.append(totals_row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summary_stats(df: pd.DataFrame, label: str) -> dict:\n",
    "    if df.empty:\n",
    "        return {\"label\": label, \"total\": 0, \"daytime\": 0, \"evening\": 0, \"top_type\": \"N/A\"}\n",
    "    day_df = df[df[\"timestamp\"].apply(is_daytime)]\n",
    "    eve_df = df[~df[\"timestamp\"].apply(is_daytime)]\n",
    "    top = df[\"category\"].value_counts()\n",
    "    return {\n",
    "        \"label\":    label,\n",
    "        \"total\":    len(df),\n",
    "        \"daytime\":  len(day_df),\n",
    "        \"evening\":  len(eve_df),\n",
    "        \"top_type\": top.index[0] if not top.empty else \"N/A\",\n",
    "    }\n",
    "\n",
    "print(\"Rolling stats functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11b710",
   "metadata": {},
   "source": [
    "## Section 3b: Download CSV from First Due SizeUp (Playwright)\n",
    "> Logs into sizeup.firstduesizeup.com and clicks the export button automatically.\n",
    "> Result is uploaded to GCS and used as the data source in Section 4b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b6e6c",
   "metadata": {},
   "outputs": [],
   "source": "import os, tempfile, subprocess, sys, json as _json\n\nDOWNLOAD_DIR = tempfile.gettempdir()\n\n# ── upload_to_gcs defined here (also used later in Section 11 for the report)\ndef upload_to_gcs(local_path: str, bucket_name: str, folder: str = \"reports\") -> str:\n    try:\n        from google.cloud import storage\n        client = storage.Client()\n        bucket = client.bucket(bucket_name)\n        blob_name = f\"{folder}/{os.path.basename(local_path)}\"\n        blob = bucket.blob(blob_name)\n        blob.upload_from_filename(local_path)\n        gcs_uri = f\"gs://{bucket_name}/{blob_name}\"\n        print(f\"Uploaded to GCS: {gcs_uri}\")\n        return gcs_uri\n    except Exception as e:\n        print(f\"GCS upload skipped: {e}\")\n        return \"\"\n\n# Write credentials to temp file (avoids f-string quoting issues)\n_creds_path = os.path.join(DOWNLOAD_DIR, \"sizeup_creds.json\")\nwith open(_creds_path, \"w\") as _f:\n    _json.dump({\n        \"email\":    FIRSTDUE_EMAIL,\n        \"password\": FIRSTDUE_PASSWORD,\n        \"dl_dir\":   DOWNLOAD_DIR,\n    }, _f)\n\n# ── Scraper script written to disk ──────────────────────────────────────────\n_scraper = os.path.join(DOWNLOAD_DIR, \"sizeup_scraper.py\")\n_scraper_src = r\"\"\"\nimport os, sys, json\nfrom playwright.sync_api import sync_playwright, TimeoutError as PWTimeout\n\nwith open(sys.argv[1]) as f:\n    cfg = json.load(f)\n\nemail    = cfg[\"email\"]\npassword = cfg[\"password\"]\ndl_dir   = cfg[\"dl_dir\"]\nshot     = os.path.join(dl_dir, \"sizeup_debug.png\")\nDISPATCH = \"https://sizeup.firstduesizeup.com/dispatch\"\nAUTH_URL = \"https://sizeup.firstduesizeup.com/auth/signin-v2\"\n\ndef dump_inputs(page, label):\n    els = page.locator(\"input\").all()\n    print(f\"[{label}] {len(els)} input(s) on page:\", flush=True)\n    for el in els:\n        t  = el.get_attribute(\"type\") or \"text\"\n        n  = el.get_attribute(\"name\") or \"\"\n        i  = el.get_attribute(\"id\")   or \"\"\n        ph = el.get_attribute(\"placeholder\") or \"\"\n        vis = el.is_visible()\n        print(f\"  type={t!r} name={n!r} id={i!r} ph={ph!r} visible={vis}\", flush=True)\n\ndef snap(page, shot, label):\n    page.screenshot(path=shot)\n    print(f\"SCREENSHOT:{shot}  [{label}]\", flush=True)\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(\n        headless=True,\n        args=[\"--no-sandbox\", \"--disable-dev-shm-usage\", \"--disable-gpu\"]\n    )\n    ctx  = browser.new_context(accept_downloads=True)\n    page = ctx.new_page()\n    page.set_default_timeout(20000)\n\n    # ── 1. Navigate ──────────────────────────────────────────────────────\n    print(f\"Navigating to {DISPATCH}\", flush=True)\n    try:\n        page.goto(DISPATCH, wait_until=\"domcontentloaded\", timeout=60000)\n    except PWTimeout:\n        pass\n    page.wait_for_timeout(3000)\n    print(f\"URL after nav: {page.url}\", flush=True)\n    snap(page, shot, \"after-nav\")\n    dump_inputs(page, \"after-nav\")\n\n    # ── 2. Detect login page ─────────────────────────────────────────────\n    on_auth = any(x in page.url for x in (\"signin\", \"auth\", \"login\"))\n    if not on_auth:\n        for pattern in (\"**signin**\", \"**auth**\", \"**login**\"):\n            try:\n                page.wait_for_url(pattern, timeout=3000)\n                on_auth = True\n                break\n            except PWTimeout:\n                pass\n    if not on_auth:\n        try:\n            page.wait_for_selector(\"input:not([type='hidden'])\", timeout=4000)\n            on_auth = True\n            print(\"Login form detected via input field (no URL change)\", flush=True)\n        except PWTimeout:\n            pass\n\n    if on_auth:\n        print(f\"On auth page: {page.url}\", flush=True)\n\n        # ── Step 1: fill email ────────────────────────────────────────\n        EMAIL_SELS = [\n            \"input[type='email']\",\n            \"input[name='email']\",\n            \"input[id='email']\",\n            \"input[name='username']\",\n            \"input[autocomplete='email']\",\n            \"input[placeholder*='mail' i]\",\n            \"input[placeholder*='user' i]\",\n        ]\n        email_el = None\n        for sel in EMAIL_SELS:\n            loc = page.locator(sel)\n            if loc.count() > 0 and loc.first.is_visible():\n                email_el = loc.first\n                print(f\"Email field matched selector: {sel}\", flush=True)\n                break\n        if email_el is None:\n            for el in page.locator(\"input\").all():\n                t = el.get_attribute(\"type\") or \"text\"\n                if t not in (\"hidden\", \"checkbox\", \"radio\", \"submit\", \"button\") and el.is_visible():\n                    email_el = el\n                    print(f\"Email field fallback: type={t!r}\", flush=True)\n                    break\n        if email_el is None:\n            dump_inputs(page, \"no-email-found\")\n            snap(page, shot, \"no-email-found\")\n            print(\"ERROR: could not find email field\", flush=True)\n            sys.exit(1)\n\n        email_el.click()\n        email_el.fill(email)\n        print(f\"Email filled.\", flush=True)\n\n        BTN_SELS = [\n            \"button[type='submit']\",\n            \"button:has-text('Continue')\",\n            \"button:has-text('Next')\",\n            \"button:has-text('Sign in')\",\n            \"button:has-text('Log in')\",\n            \"input[type='submit']\",\n        ]\n        clicked = False\n        for sel in BTN_SELS:\n            loc = page.locator(sel)\n            if loc.count() > 0 and loc.first.is_visible():\n                print(f\"Clicking continue button: {sel}\", flush=True)\n                loc.first.click()\n                clicked = True\n                break\n        if not clicked:\n            print(\"WARNING: no Continue button — pressing Enter\", flush=True)\n            email_el.press(\"Enter\")\n\n        # ── Step 2: wait for password field ──────────────────────────\n        print(\"Waiting for password field...\", flush=True)\n        try:\n            page.wait_for_selector(\"input[type='password']:visible\", timeout=10000)\n            print(\"Password field appeared.\", flush=True)\n        except PWTimeout:\n            snap(page, shot, \"no-password-field\")\n            dump_inputs(page, \"no-password-field\")\n            print(\"ERROR: password field did not appear\", flush=True)\n            sys.exit(1)\n\n        page.wait_for_timeout(500)\n        snap(page, shot, \"password-step\")\n\n        pw_el = page.locator(\"input[type='password']:visible\").first\n        pw_el.click()\n        pw_el.fill(password)\n        print(\"Password filled.\", flush=True)\n\n        # ── Submit ────────────────────────────────────────────────────\n        # Use the button whose text is specifically 'Sign in' or 'Log in'\n        # to avoid accidentally re-clicking 'Continue'\n        SUBMIT_SELS = [\n            \"button:has-text('Sign in')\",\n            \"button:has-text('Log in')\",\n            \"button:has-text('Submit')\",\n            \"button[type='submit']\",\n            \"input[type='submit']\",\n        ]\n        clicked2 = False\n        for sel in SUBMIT_SELS:\n            loc = page.locator(sel)\n            if loc.count() > 0 and loc.first.is_visible():\n                print(f\"Submit clicking: {sel}\", flush=True)\n                loc.first.click()\n                clicked2 = True\n                break\n        if not clicked2:\n            print(\"WARNING: no submit button — pressing Enter on password\", flush=True)\n            pw_el.press(\"Enter\")\n\n        # ── Verify login succeeded (URL must leave auth page) ─────────\n        print(\"Verifying login success (waiting for URL to leave auth page)...\", flush=True)\n        try:\n            page.wait_for_function(\n                \"() => !window.location.href.includes('signin') && \"\n                \"       !window.location.href.includes('/auth/')\",\n                timeout=20000\n            )\n            print(f\"Login confirmed! URL: {page.url}\", flush=True)\n        except PWTimeout:\n            snap(page, shot, \"login-failed\")\n            dump_inputs(page, \"login-failed\")\n            print(\n                f\"WARNING: Login did not complete — URL still {page.url}.\\n\"\n                \"This may be caused by bot/datacenter IP detection on Cloud Run.\\n\"\n                \"The report will fall back to the most recent CSV in GCS.\",\n                flush=True\n            )\n            ctx.close()\n            browser.close()\n            sys.exit(2)   # exit code 2 = soft fail (fallback to GCS)\n\n        page.wait_for_timeout(3000)\n        snap(page, shot, \"post-login\")\n\n    else:\n        print(\"Already authenticated — no login needed.\", flush=True)\n        snap(page, shot, \"already-authed\")\n\n    # ── 3. Download CSV ──────────────────────────────────────────────────\n    print(\"Waiting for download button...\", flush=True)\n    DL_BTN = \"[data-testid='button_download_csv']\"\n    try:\n        page.wait_for_selector(DL_BTN, timeout=30000)\n        print(\"Download button found!\", flush=True)\n    except PWTimeout as e:\n        snap(page, shot, \"no-download-btn\")\n        try:\n            testids = page.evaluate(\n                \"() => [...document.querySelectorAll('[data-testid]')]\"\n                \".map(el => el.getAttribute('data-testid')).slice(0, 40)\"\n            )\n            print(f\"data-testid elements on page: {testids}\", flush=True)\n        except Exception:\n            pass\n        print(f\"ERROR waiting for download button: {e}\", flush=True)\n        sys.exit(1)\n\n    snap(page, shot, \"ready-to-download\")\n    with page.expect_download(timeout=60000) as dl_info:\n        page.click(DL_BTN)\n    download = dl_info.value\n    out = os.path.join(dl_dir, download.suggested_filename or \"D90.csv\")\n    download.save_as(out)\n    print(f\"DOWNLOADED:{out}\", flush=True)\n\n    ctx.close()\n    browser.close()\n\"\"\"\n\nwith open(_scraper, \"w\") as _f:\n    _f.write(_scraper_src)\n\n# ── Run ─────────────────────────────────────────────────────────────────────\nscraped_csv     = None\nscreenshot_path = None\n\ntry:\n    res = subprocess.run(\n        [sys.executable, _scraper, _creds_path],\n        capture_output=True, text=True, timeout=180\n    )\n    print(\"=== STDOUT ===\")\n    print(res.stdout)\n    if res.stderr:\n        print(\"=== STDERR (last 2000) ===\")\n        print(res.stderr[-2000:])\n\n    for line in res.stdout.splitlines():\n        if line.startswith(\"DOWNLOADED:\"):\n            scraped_csv = line.split(\"DOWNLOADED:\", 1)[1].strip()\n        if line.startswith(\"SCREENSHOT:\"):\n            screenshot_path = line.split(\"SCREENSHOT:\", 1)[1].split(\"[\")[0].strip()\n\n    if screenshot_path and os.path.exists(screenshot_path):\n        from IPython.display import Image, display\n        display(Image(filename=screenshot_path))\n\n    if scraped_csv and os.path.exists(scraped_csv):\n        print(f\"\\nCSV ready: {scraped_csv} ({os.path.getsize(scraped_csv):,} bytes)\")\n        upload_to_gcs(scraped_csv, GCS_BUCKET_NAME, folder=\"data\")\n        GCS_CSV_PATH = \"data/\" + os.path.basename(scraped_csv)\n    else:\n        exit_code = res.returncode\n        if exit_code == 2:\n            print(\"\\nScraper soft-failed (likely Cloud Run IP block) — will use GCS fallback.\")\n        else:\n            print(f\"\\nScraper failed (exit {exit_code}) — will use GCS fallback.\")\n        scraped_csv = None\nfinally:\n    if os.path.exists(_creds_path):\n        os.remove(_creds_path)"
  },
  {
   "cell_type": "markdown",
   "id": "69d16d22",
   "metadata": {},
   "source": [
    "## Section 4b: Load from CSV (Active Data Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18637478",
   "metadata": {},
   "outputs": [],
   "source": "# Default GCS path — overridden by scraper cell if a fresh download succeeded\nGCS_CSV_PATH   = os.environ.get(\"GCS_CSV_PATH\", \"data/D90.csv\")\nLOCAL_CSV_PATH = \"D90.csv\"\n\ndef download_csv_from_gcs(bucket_name: str, blob_name: str, local_dest: str) -> str:\n    try:\n        from google.cloud import storage\n        client = storage.Client()\n        bucket = client.bucket(bucket_name)\n        blob   = bucket.blob(blob_name)\n        blob.download_to_filename(local_dest)\n        print(f\"Downloaded gs://{bucket_name}/{blob_name} -> {local_dest}\")\n        return local_dest\n    except Exception as e:\n        print(f\"GCS download failed: {e}\")\n        return None\n\ndef load_from_csv(csv_path: str) -> pd.DataFrame:\n    df = pd.read_csv(csv_path)\n    df.columns = [c.strip() for c in df.columns]\n    date_col = next((c for c in df.columns if \"DATE\" in c.upper() or \"TIME\" in c.upper()), None)\n    if date_col is None:\n        raise ValueError(f\"No date column found. Columns: {list(df.columns)}\")\n    df[\"timestamp\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[\"timestamp\"])\n    df[\"timestamp\"] = df[\"timestamp\"].apply(\n        lambda x: x.replace(tzinfo=None) if x.tzinfo else x\n    )\n    type_col = next((c for c in df.columns if \"TYPE\" in c.upper()), None)\n    addr_col = next((c for c in df.columns if \"ADDRESS\" in c.upper()), None)\n    df[\"incident_type\"] = df[type_col].fillna(\"Unknown\") if type_col else \"Unknown\"\n    df[\"address\"]       = df[addr_col].fillna(\"\")        if addr_col else \"\"\n    df[\"category\"]      = df[\"incident_type\"].apply(categorize)\n    print(f\"Loaded {len(df)} incidents from {csv_path}\")\n    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n    cat_counts = df[\"category\"].value_counts()\n    print(\"Category breakdown:\")\n    for cat, cnt in cat_counts.items():\n        print(f\"  {cat}: {cnt}\")\n    return df\n\n# ── Pick the best available CSV source (priority order) ──────────────────\n# 1. Freshly scraped from SizeUp this run (scraped_csv set by Section 3b)\n# 2. Download latest from GCS  (uploaded by previous scraper runs)\n# 3. Local fallback D90.csv\n\nimport tempfile as _tmpfile, os as _os\n\ncsv_to_load = None\n\n# 1. Scraped this run?\n_scraped = globals().get(\"scraped_csv\")\nif _scraped and _os.path.exists(_scraped):\n    csv_to_load = _scraped\n    print(f\"Using freshly scraped CSV: {csv_to_load}\")\n\n# 2. Try GCS\nif csv_to_load is None:\n    _gcs_dest = _os.path.join(_tmpfile.gettempdir(), \"gcs_D90.csv\")\n    _dl = download_csv_from_gcs(GCS_BUCKET_NAME, GCS_CSV_PATH, _gcs_dest)\n    if _dl:\n        csv_to_load = _dl\n        print(f\"Using GCS CSV: {GCS_CSV_PATH}\")\n\n# 3. Local fallback\nif csv_to_load is None:\n    csv_to_load = LOCAL_CSV_PATH\n    print(f\"Falling back to local: {csv_to_load}\")\n\nprint(f\"Loading from: {csv_to_load}\")\ndf_all = load_from_csv(csv_to_load)\n\ndf_daily   = filter_window(df_all, DAILY_START)\ndf_weekly  = filter_window(df_all, WEEKLY_START)\ndf_monthly = filter_window(df_all, MONTHLY_START)\n\ntable_daily   = build_incident_table(df_daily)\ntable_weekly  = build_incident_table(df_weekly)\ntable_monthly = build_incident_table(df_monthly)\n\nstats_daily   = summary_stats(df_daily,   \"Last 24 Hours\")\nstats_weekly  = summary_stats(df_weekly,  \"Last 7 Days\")\nstats_monthly = summary_stats(df_monthly, \"Last 30 Days\")\n\nprint(f\"\\nWindow counts:\")\nprint(f\"  Daily:   {stats_daily['total']} incidents\")\nprint(f\"  Weekly:  {stats_weekly['total']} incidents\")\nprint(f\"  Monthly: {stats_monthly['total']} incidents\")"
  },
  {
   "cell_type": "markdown",
   "id": "7a58ce38",
   "metadata": {},
   "source": [
    "## Section 5b: Incident Histogram (10-Day Buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca729eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "def plot_incident_histogram(df, title=\"Station 187 — Incidents by 10-Day Period\"):\n",
    "    if df.empty:\n",
    "        print(\"No data to plot.\")\n",
    "        return None\n",
    "\n",
    "    df = df.copy()\n",
    "    min_date = df[\"timestamp\"].min().floor(\"D\")\n",
    "    max_date = df[\"timestamp\"].max().ceil(\"D\")\n",
    "\n",
    "    # Build 10-day bin edges\n",
    "    bin_edges = []\n",
    "    cursor = min_date\n",
    "    while cursor <= max_date:\n",
    "        bin_edges.append(cursor)\n",
    "        cursor += pd.Timedelta(days=10)\n",
    "    if bin_edges[-1] < max_date:\n",
    "        bin_edges.append(cursor)\n",
    "\n",
    "    bin_labels = [f\"{b.strftime('%b %d')}\" for b in bin_edges[:-1]]\n",
    "    df[\"bucket\"] = pd.cut(df[\"timestamp\"], bins=bin_edges, labels=bin_labels, right=False)\n",
    "    df = df.dropna(subset=[\"bucket\"])\n",
    "\n",
    "    # Top categories by volume\n",
    "    top_cats = df[\"category\"].value_counts().head(12).index.tolist()\n",
    "    df[\"cat_display\"] = df[\"category\"].apply(lambda x: x if x in top_cats else \"Other / Uncategorized\")\n",
    "\n",
    "    all_cats = top_cats.copy()\n",
    "    if \"Other / Uncategorized\" not in all_cats and (df[\"cat_display\"] == \"Other / Uncategorized\").any():\n",
    "        all_cats.append(\"Other / Uncategorized\")\n",
    "\n",
    "    pivot = (\n",
    "        df.groupby([\"bucket\", \"cat_display\"], observed=True)\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(columns=all_cats, fill_value=0)\n",
    "    )\n",
    "\n",
    "    # Color palette\n",
    "    palette = [\n",
    "        \"#C0392B\", \"#2980B9\", \"#27AE60\", \"#F39C12\", \"#8E44AD\",\n",
    "        \"#16A085\", \"#D35400\", \"#2C3E50\", \"#1ABC9C\", \"#E74C3C\",\n",
    "        \"#3498DB\", \"#F1C40F\", \"#7F8C8D\", \"#E67E22\"\n",
    "    ]\n",
    "    colors = palette[:len(pivot.columns)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    pivot.plot(\n",
    "        kind=\"bar\", stacked=True, ax=ax,\n",
    "        color=colors, width=0.78, edgecolor=\"white\", linewidth=0.4\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\", pad=14)\n",
    "    ax.set_xlabel(\"10-Day Period (Start Date)\", fontsize=11)\n",
    "    ax.set_ylabel(\"Number of Incidents\", fontsize=11)\n",
    "    ax.set_xticklabels(bin_labels, rotation=40, ha=\"right\", fontsize=9)\n",
    "    ax.legend(\n",
    "        title=\"Incident Category\", bbox_to_anchor=(1.01, 1),\n",
    "        loc=\"upper left\", fontsize=8, title_fontsize=9\n",
    "    )\n",
    "    ax.grid(axis=\"y\", alpha=0.25, linestyle=\"--\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_facecolor(\"#F9F9F9\")\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    chart_path = os.path.join(tempfile.gettempdir(), \"incident_histogram.png\")\n",
    "    plt.savefig(chart_path, dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "    print(f\"Chart saved: {chart_path}\")\n",
    "    return chart_path\n",
    "\n",
    "chart_path = plot_incident_histogram(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5465dd",
   "metadata": {},
   "source": [
    "## Section 6: Word Document Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cell_bg(cell, hex_color: str):\n",
    "    tc = cell._tc\n",
    "    tcPr = tc.get_or_add_tcPr()\n",
    "    shd = OxmlElement(\"w:shd\")\n",
    "    shd.set(qn(\"w:val\"),   \"clear\")\n",
    "    shd.set(qn(\"w:color\"), \"auto\")\n",
    "    shd.set(qn(\"w:fill\"),  hex_color)\n",
    "    tcPr.append(shd)\n",
    "\n",
    "def set_cell_border(cell, **kwargs):\n",
    "    tc = cell._tc\n",
    "    tcPr = tc.get_or_add_tcPr()\n",
    "    tcBorders = OxmlElement(\"w:tcBorders\")\n",
    "    for edge in (\"top\", \"left\", \"bottom\", \"right\", \"insideH\", \"insideV\"):\n",
    "        tag = OxmlElement(f\"w:{edge}\")\n",
    "        tag.set(qn(\"w:val\"),   kwargs.get(\"val\",   \"single\"))\n",
    "        tag.set(qn(\"w:sz\"),    kwargs.get(\"sz\",    \"4\"))\n",
    "        tag.set(qn(\"w:space\"), kwargs.get(\"space\", \"0\"))\n",
    "        tag.set(qn(\"w:color\"), kwargs.get(\"color\", \"000000\"))\n",
    "        tcBorders.append(tag)\n",
    "    tcPr.append(tcBorders)\n",
    "\n",
    "def add_incident_table_to_doc(doc, df_table: pd.DataFrame, title: str):\n",
    "    doc.add_heading(title, level=2)\n",
    "    if df_table.empty or len(df_table) == 0:\n",
    "        doc.add_paragraph(\"No incidents in this window.\")\n",
    "        return\n",
    "    cols = [\"Category\", \"Daytime\", \"Evening/Night\", \"Total\"]\n",
    "    table = doc.add_table(rows=1, cols=len(cols))\n",
    "    table.style = \"Table Grid\"\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col in enumerate(cols):\n",
    "        hdr_cells[i].text = col\n",
    "        hdr_cells[i].paragraphs[0].runs[0].bold = True\n",
    "        hdr_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        set_cell_bg(hdr_cells[i], \"1F3864\")\n",
    "        for run in hdr_cells[i].paragraphs[0].runs:\n",
    "            run.font.color.rgb = RGBColor(0xFF, 0xFF, 0xFF)\n",
    "            run.font.size = Pt(9)\n",
    "        set_cell_border(hdr_cells[i])\n",
    "    for _, row in df_table.iterrows():\n",
    "        is_total = str(row[\"Category\"]).upper() == \"TOTAL\"\n",
    "        cells = table.add_row().cells\n",
    "        cells[0].text = str(row[\"Category\"])\n",
    "        cells[1].text = str(row[\"Daytime\"])\n",
    "        cells[2].text = str(row[\"Evening/Night\"])\n",
    "        cells[3].text = str(row[\"Total\"])\n",
    "        for j, cell in enumerate(cells):\n",
    "            para = cell.paragraphs[0]\n",
    "            para.alignment = WD_ALIGN_PARAGRAPH.CENTER if j > 0 else WD_ALIGN_PARAGRAPH.LEFT\n",
    "            if is_total:\n",
    "                set_cell_bg(cell, \"D9E1F2\")\n",
    "                for run in para.runs:\n",
    "                    run.bold = True\n",
    "                    run.font.size = Pt(9)\n",
    "            else:\n",
    "                for run in para.runs:\n",
    "                    run.font.size = Pt(9)\n",
    "            set_cell_border(cell)\n",
    "\n",
    "def add_mutual_aid_section(doc, df: pd.DataFrame):\n",
    "    doc.add_heading(\"Mutual Aid Summary\", level=2)\n",
    "    ma_given    = df[df[\"category\"] == \"Mutual Aid Given\"]\n",
    "    ma_received = df[df[\"category\"] == \"Mutual Aid Received\"]\n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run(f\"Mutual Aid Given:    \").bold = True\n",
    "    p.add_run(str(len(ma_given)))\n",
    "    p2 = doc.add_paragraph()\n",
    "    p2.add_run(f\"Mutual Aid Received: \").bold = True\n",
    "    p2.add_run(str(len(ma_received)))\n",
    "\n",
    "def add_definitions_section(doc):\n",
    "    doc.add_heading(\"Definitions\", level=2)\n",
    "    definitions = [\n",
    "        (\"Daytime\",       f\"{DAYTIME_START_HOUR:02d}:00 - {DAYTIME_END_HOUR:02d}:00\"),\n",
    "        (\"Evening/Night\", f\"{DAYTIME_END_HOUR:02d}:00 - {DAYTIME_START_HOUR:02d}:00\"),\n",
    "        (\"AFA\",           \"Automatic Fire Alarm\"),\n",
    "        (\"Commercial AFA\",\"Fire alarm activation in a commercial occupancy\"),\n",
    "        (\"Residential AFA\",\"Fire alarm activation in a residential occupancy\"),\n",
    "    ]\n",
    "    for term, defn in definitions:\n",
    "        p = doc.add_paragraph(style=\"List Bullet\")\n",
    "        p.add_run(f\"{term}: \").bold = True\n",
    "        p.add_run(defn)\n",
    "\n",
    "def build_word_document(\n",
    "    table_daily, table_weekly, table_monthly,\n",
    "    df_daily, df_weekly, df_monthly,\n",
    "    weather_data: dict, traffic_data: dict,\n",
    "    narrative: str,\n",
    "    output_path: str,\n",
    "    chart_path: str = None,\n",
    "):\n",
    "    doc = Document()\n",
    "    style = doc.styles[\"Normal\"]\n",
    "    style.font.name = \"Calibri\"\n",
    "    style.font.size = Pt(11)\n",
    "\n",
    "    # Title\n",
    "    title = doc.add_heading(\"Ingomar Volunteer Fire Company\", level=1)\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    sub = doc.add_heading(\"Station 187 - Daily Operations Report\", level=2)\n",
    "    sub.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    date_p = doc.add_paragraph(NOW_ET.strftime(\"%A, %B %d, %Y  |  Generated at %H:%M %Z\"))\n",
    "    date_p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    doc.add_paragraph()\n",
    "\n",
    "    # Risk scores\n",
    "    doc.add_heading(\"Risk Assessment\", level=2)\n",
    "    risk_table = doc.add_table(rows=1, cols=3)\n",
    "    risk_table.style = \"Table Grid\"\n",
    "    hcells = risk_table.rows[0].cells\n",
    "    for i, label in enumerate([\"Metric\", \"Score (1-10)\", \"Details\"]):\n",
    "        hcells[i].text = label\n",
    "        hcells[i].paragraphs[0].runs[0].bold = True\n",
    "        set_cell_bg(hcells[i], \"1F3864\")\n",
    "        for run in hcells[i].paragraphs[0].runs:\n",
    "            run.font.color.rgb = RGBColor(0xFF, 0xFF, 0xFF)\n",
    "\n",
    "    for row_data in [\n",
    "        (\"Weather Risk\", weather_data.get(\"risk_score\", \"N/A\"), weather_data.get(\"summary\", \"\")),\n",
    "        (\"Traffic Risk\", traffic_data.get(\"risk_score\", \"N/A\"), traffic_data.get(\"summary\", \"\")),\n",
    "    ]:\n",
    "        cells = risk_table.add_row().cells\n",
    "        for j, val in enumerate(row_data):\n",
    "            cells[j].text = str(val)\n",
    "            set_cell_border(cells[j])\n",
    "\n",
    "    doc.add_paragraph()\n",
    "\n",
    "    # Duty crew narrative\n",
    "    doc.add_heading(\"Duty Crew Risk Narrative\", level=2)\n",
    "    doc.add_paragraph(narrative)\n",
    "    doc.add_paragraph()\n",
    "\n",
    "    # Incident tables\n",
    "    add_incident_table_to_doc(doc, table_daily,   \"Last 24 Hours - Incident Summary\")\n",
    "    doc.add_paragraph()\n",
    "    add_incident_table_to_doc(doc, table_weekly,  \"Last 7 Days - Incident Summary\")\n",
    "    doc.add_paragraph()\n",
    "    add_incident_table_to_doc(doc, table_monthly, \"Last 30 Days - Incident Summary\")\n",
    "    doc.add_paragraph()\n",
    "\n",
    "    # Mutual aid\n",
    "    add_mutual_aid_section(doc, df_monthly)\n",
    "    doc.add_paragraph()\n",
    "\n",
    "    # Definitions\n",
    "    add_definitions_section(doc)\n",
    "\n",
    "\n",
    "    # Incident histogram chart\n",
    "    if chart_path and os.path.exists(chart_path):\n",
    "        doc.add_heading(\"Incident Distribution (10-Day Buckets)\", level=2)\n",
    "        doc.add_picture(chart_path, width=Inches(6.5))\n",
    "        doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        doc.add_paragraph()\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Word document saved: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "print(\"Word document builder functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eae097",
   "metadata": {},
   "source": [
    "## Section 7: Weather Risk Score (NWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NWS_HEADERS = {\"User-Agent\": \"IVFC-DailyReport/1.0 (contact@ivfc187.org)\"}\n",
    "\n",
    "def get_pittsburgh_weather() -> dict:\n",
    "    try:\n",
    "        obs_url = \"https://api.weather.gov/stations/KPIT/observations/latest\"\n",
    "        obs = requests.get(obs_url, headers=NWS_HEADERS, timeout=15).json()\n",
    "        props = obs.get(\"properties\", {})\n",
    "        temp_c    = props.get(\"temperature\", {}).get(\"value\")\n",
    "        wind_ms   = props.get(\"windSpeed\",   {}).get(\"value\")\n",
    "        precip    = props.get(\"precipitationLastHour\", {}).get(\"value\") or 0\n",
    "        vis_m     = props.get(\"visibility\",  {}).get(\"value\")\n",
    "        conditions = str(props.get(\"textDescription\", \"\")).lower()\n",
    "\n",
    "        temp_f  = (temp_c * 9/5 + 32) if temp_c is not None else None\n",
    "        wind_mph = (wind_ms * 2.237)   if wind_ms is not None else 0\n",
    "        vis_mi  = (vis_m / 1609.34)    if vis_m is not None else 10\n",
    "\n",
    "        score = 1\n",
    "        if temp_f is not None:\n",
    "            if temp_f <= 20 or temp_f >= 100:\n",
    "                score += 3\n",
    "            elif temp_f <= 32 or temp_f >= 90:\n",
    "                score += 2\n",
    "            elif temp_f <= 40 or temp_f >= 85:\n",
    "                score += 1\n",
    "        if wind_mph >= 40:\n",
    "            score += 3\n",
    "        elif wind_mph >= 25:\n",
    "            score += 2\n",
    "        elif wind_mph >= 15:\n",
    "            score += 1\n",
    "        if precip and precip > 0:\n",
    "            score += 2\n",
    "        elif any(w in conditions for w in [\"rain\", \"snow\", \"storm\", \"thunder\", \"sleet\", \"ice\"]):\n",
    "            score += 2\n",
    "        if vis_mi < 0.25:\n",
    "            score += 2\n",
    "        elif vis_mi < 1:\n",
    "            score += 1\n",
    "\n",
    "        score = min(score, 10)\n",
    "        summary = (\n",
    "            f\"Temp: {temp_f:.0f}F, Wind: {wind_mph:.0f} mph, \"\n",
    "            f\"Precip: {precip:.2f} in, Vis: {vis_mi:.1f} mi\"\n",
    "            if temp_f is not None else conditions\n",
    "        )\n",
    "        return {\"risk_score\": score, \"summary\": summary, \"raw\": props}\n",
    "    except Exception as e:\n",
    "        return {\"risk_score\": 5, \"summary\": f\"Weather data unavailable: {e}\", \"raw\": {}}\n",
    "\n",
    "weather_data = get_pittsburgh_weather()\n",
    "print(f\"Weather risk score: {weather_data['risk_score']}/10\")\n",
    "print(f\"Summary: {weather_data['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba98775",
   "metadata": {},
   "source": [
    "## Section 8: Traffic Risk Score (TomTom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62074e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGH_BBOX = {\n",
    "    \"min_lat\": 40.30, \"max_lat\": 40.65,\n",
    "    \"min_lon\": -80.20, \"max_lon\": -79.75,\n",
    "}\n",
    "\n",
    "def get_pittsburgh_traffic() -> dict:\n",
    "    if not TOMTOM_API_KEY:\n",
    "        return {\"risk_score\": 5, \"summary\": \"TomTom API key not set\", \"incidents\": []}\n",
    "    try:\n",
    "        bbox_str = (\n",
    "            f\"{PGH_BBOX['min_lat']},{PGH_BBOX['min_lon']},\"\n",
    "            f\"{PGH_BBOX['max_lat']},{PGH_BBOX['max_lon']}\"\n",
    "        )\n",
    "        url = (\n",
    "            f\"https://api.tomtom.com/traffic/services/5/incidentDetails\"\n",
    "            f\"?key={TOMTOM_API_KEY}&bbox={bbox_str}&fields={{incidents{{type,geometry{{type}},\"\n",
    "            f\"properties{{iconCategory,magnitudeOfDelay,startTime,endTime,length,delay,\"\n",
    "            f\"roadNumbers,timeValidity}}}}}}&language=en-GB&t=1111&categoryFilter=0,1,2,3,4,5,6,7,8,9,10,11,14\"\n",
    "        )\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            incidents = data.get(\"incidents\", [])\n",
    "            total   = len(incidents)\n",
    "            serious = sum(\n",
    "                1 for i in incidents\n",
    "                if i.get(\"properties\", {}).get(\"magnitudeOfDelay\", 0) >= 3\n",
    "            )\n",
    "            score = 1\n",
    "            if total >= 20:\n",
    "                score += 3\n",
    "            elif total >= 10:\n",
    "                score += 2\n",
    "            elif total >= 5:\n",
    "                score += 1\n",
    "            if serious >= 5:\n",
    "                score += 3\n",
    "            elif serious >= 2:\n",
    "                score += 2\n",
    "            elif serious >= 1:\n",
    "                score += 1\n",
    "            hour = NOW_ET.hour\n",
    "            if 7 <= hour <= 9 or 16 <= hour <= 18:\n",
    "                score += 2\n",
    "            elif 6 <= hour <= 10 or 15 <= hour <= 19:\n",
    "                score += 1\n",
    "            score = min(score, 10)\n",
    "            summary = f\"{total} incidents ({serious} serious) in Pittsburgh metro\"\n",
    "            return {\"risk_score\": score, \"summary\": summary, \"incidents\": incidents}\n",
    "        else:\n",
    "            hour = NOW_ET.hour\n",
    "            if 7 <= hour <= 9 or 16 <= hour <= 18:\n",
    "                score, summary = 7, \"Peak rush hour - elevated traffic risk (API unavailable)\"\n",
    "            elif 6 <= hour <= 10 or 15 <= hour <= 19:\n",
    "                score, summary = 5, \"Shoulder rush hour - moderate traffic risk (API unavailable)\"\n",
    "            else:\n",
    "                score, summary = 2, \"Off-peak hours - low traffic risk (API unavailable)\"\n",
    "            return {\"risk_score\": score, \"summary\": summary, \"incidents\": []}\n",
    "    except Exception as e:\n",
    "        return {\"risk_score\": 4, \"summary\": f\"Traffic data error: {e}\", \"incidents\": []}\n",
    "\n",
    "traffic_data = get_pittsburgh_traffic()\n",
    "print(f\"Traffic risk score: {traffic_data['risk_score']}/10\")\n",
    "print(f\"Summary: {traffic_data['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf463b",
   "metadata": {},
   "source": [
    "## Section 9: Claude AI Risk Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_risk_narrative(weather: dict, traffic: dict, df_recent: pd.DataFrame) -> str:\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "    recent_types = \"\"\n",
    "    if not df_recent.empty:\n",
    "        top = df_recent[\"category\"].value_counts().head(5)\n",
    "        recent_types = \", \".join([f\"{cat} ({cnt})\" for cat, cnt in top.items()])\n",
    "    else:\n",
    "        recent_types = \"No incidents in the last 24 hours\"\n",
    "\n",
    "    prompt = f\"\"\"You are a fire department operations analyst for Ingomar Volunteer Fire Company, Station 187,\n",
    "serving the McCandless area of Allegheny County, Pennsylvania.\n",
    "\n",
    "Current conditions:\n",
    "- Weather risk score: {weather.get('risk_score', 'N/A')}/10\n",
    "- Weather summary: {weather.get('summary', 'N/A')}\n",
    "- Traffic risk score: {traffic.get('risk_score', 'N/A')}/10\n",
    "- Traffic summary: {traffic.get('summary', 'N/A')}\n",
    "- Recent 24-hour incident types: {recent_types}\n",
    "\n",
    "Write a concise duty-crew risk narrative (3-4 sentences) for today operations briefing.\n",
    "Include: the highest current risk factor, what incident types the crew should be prepared for,\n",
    "and one specific mitigation recommendation. Use professional fire service language.\"\"\"\n",
    "\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-haiku-4-5-20251001\",\n",
    "            max_tokens=300,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"Risk narrative unavailable ({e}). \"\n",
    "            f\"Weather risk: {weather.get('risk_score')}/10. \"\n",
    "            f\"Traffic risk: {traffic.get('risk_score')}/10. \"\n",
    "            f\"Primary incident type: {df_recent['category'].value_counts().index[0] if not df_recent.empty else 'N/A'}.\"\n",
    "        )\n",
    "\n",
    "narrative = generate_risk_narrative(weather_data, traffic_data, df_daily)\n",
    "print(\"Risk narrative generated.\")\n",
    "print()\n",
    "print(narrative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4d784",
   "metadata": {},
   "source": [
    "## Section 10: Assemble and Save Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile\n",
    "\n",
    "report_filename = f\"IVFC_DailyReport_{NOW_ET.strftime('%Y%m%d_%H%M')}.docx\"\n",
    "report_path = os.path.join(tempfile.gettempdir(), report_filename)\n",
    "\n",
    "build_word_document(\n",
    "    table_daily   = table_daily,\n",
    "    table_weekly  = table_weekly,\n",
    "    table_monthly = table_monthly,\n",
    "    df_daily      = df_daily,\n",
    "    df_weekly     = df_weekly,\n",
    "    df_monthly    = df_monthly,\n",
    "    weather_data  = weather_data,\n",
    "    traffic_data  = traffic_data,\n",
    "    narrative     = narrative,\n",
    "    output_path   = report_path,\n",
    "    chart_path    = chart_path if \"chart_path\" in dir() else None,\n",
    ")\n",
    "\n",
    "print(f\"Report assembled: {report_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc53d5",
   "metadata": {},
   "source": [
    "## Section 11: Upload to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af602c",
   "metadata": {},
   "outputs": [],
   "source": "# upload_to_gcs is defined in Section 3b (before the scraper) so it's available early.\n# Calling it here to upload the final Word report.\ngcs_uri = upload_to_gcs(report_path, GCS_BUCKET_NAME)"
  },
  {
   "cell_type": "markdown",
   "id": "5fa4c213",
   "metadata": {},
   "source": [
    "## Section 12: Email Report via SendGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_report_email(\n",
    "        report_path: str,\n",
    "        report_filename: str,\n",
    "        gcs_uri: str,\n",
    "        weather: dict,\n",
    "        traffic: dict,\n",
    ") -> bool:\n",
    "    try:\n",
    "        import sendgrid\n",
    "        from sendgrid.helpers.mail import (\n",
    "            Mail, Attachment, FileContent, FileName,\n",
    "            FileType, Disposition, To,\n",
    "        )\n",
    "        to_list = [addr.strip() for addr in EMAIL_TO.split(\",\") if addr.strip()]\n",
    "        with open(report_path, \"rb\") as f:\n",
    "            file_data = base64.b64encode(f.read()).decode()\n",
    "        body = (\n",
    "            f\"IVFC Station 187 - Daily Operations Report\\n\\n\"\n",
    "            f\"Report Date: {NOW_ET.strftime('%A, %B %d, %Y')}\\n\"\n",
    "            f\"Generated:   {NOW_ET.strftime('%H:%M %Z')}\\n\\n\"\n",
    "            f\"Weather Risk Score: {weather.get('risk_score', 'N/A')}/10 - {weather.get('summary', '')}\\n\"\n",
    "            f\"Traffic Risk Score: {traffic.get('risk_score', 'N/A')}/10 - {traffic.get('summary', '')}\\n\"\n",
    "        )\n",
    "        if gcs_uri:\n",
    "            body += f\"\\nGCS Location: {gcs_uri}\\n\"\n",
    "        message = Mail(\n",
    "            from_email = EMAIL_FROM,\n",
    "            to_emails  = [To(addr) for addr in to_list],\n",
    "            subject    = f\"IVFC Station 187 Daily Report - {NOW_ET.strftime('%B %d, %Y')}\",\n",
    "            plain_text_content = body,\n",
    "        )\n",
    "        attachment = Attachment(\n",
    "            FileContent(file_data),\n",
    "            FileName(report_filename),\n",
    "            FileType(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"),\n",
    "            Disposition(\"attachment\"),\n",
    "        )\n",
    "        message.attachment = attachment\n",
    "        sg = sendgrid.SendGridAPIClient(api_key=SENDGRID_API_KEY)\n",
    "        response = sg.send(message)\n",
    "        print(f\"Email sent. Status: {response.status_code}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"SendGrid failed ({e}), trying SMTP fallback...\")\n",
    "        try:\n",
    "            import smtplib\n",
    "            from email.mime.multipart import MIMEMultipart\n",
    "            from email.mime.base import MIMEBase\n",
    "            from email.mime.text import MIMEText\n",
    "            from email import encoders\n",
    "            msg = MIMEMultipart()\n",
    "            msg[\"From\"]    = EMAIL_FROM\n",
    "            msg[\"To\"]      = EMAIL_TO\n",
    "            msg[\"Subject\"] = f\"IVFC Station 187 Daily Report - {NOW_ET.strftime('%B %d, %Y')}\"\n",
    "            msg.attach(MIMEText(body, \"plain\"))\n",
    "            with open(report_path, \"rb\") as f:\n",
    "                part = MIMEBase(\"application\", \"octet-stream\")\n",
    "                part.set_payload(f.read())\n",
    "                encoders.encode_base64(part)\n",
    "                part.add_header(\"Content-Disposition\", f'attachment; filename=\"{report_filename}\"')\n",
    "                msg.attach(part)\n",
    "            with smtplib.SMTP(\"smtp.sendgrid.net\", 587) as server:\n",
    "                server.starttls()\n",
    "                server.login(\"apikey\", SENDGRID_API_KEY)\n",
    "                server.sendmail(EMAIL_FROM, EMAIL_TO.split(\",\"), msg.as_string())\n",
    "            print(\"Email sent via SMTP fallback.\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"Email failed entirely: {e2}\")\n",
    "            return False\n",
    "\n",
    "email_sent = send_report_email(\n",
    "    report_path      = report_path,\n",
    "    report_filename  = report_filename,\n",
    "    gcs_uri          = gcs_uri,\n",
    "    weather          = weather_data,\n",
    "    traffic          = traffic_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb50bcf",
   "metadata": {},
   "source": [
    "## Section 13: Execution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  IVFC DAILY OPERATIONS REPORT - EXECUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Report date:     {NOW_ET.strftime('%A, %B %d, %Y')}\")\n",
    "print(f\"  Generated at:    {NOW_ET.strftime('%H:%M %Z')}\")\n",
    "print()\n",
    "print(f\"  Incidents (24h): {stats_daily['total']}\")\n",
    "print(f\"  Incidents (7d):  {stats_weekly['total']}\")\n",
    "print(f\"  Incidents (30d): {stats_monthly['total']}\")\n",
    "print()\n",
    "print(f\"  Weather risk:    {weather_data.get('risk_score', 'N/A')}/10\")\n",
    "print(f\"  Traffic risk:    {traffic_data.get('risk_score', 'N/A')}/10\")\n",
    "print()\n",
    "print(f\"  Report file:     {report_filename}\")\n",
    "print(f\"  GCS location:    {gcs_uri if gcs_uri else 'Not uploaded'}\")\n",
    "print(f\"  Email sent:      {'Yes' if email_sent else 'No'}\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}